# [算能杯——面向Stable Diffusion的图像提示语优化专项赛](https://www.saikr.com/vse/48028)

目标为创建一个模型来预测给定生成图像的文本提示。参赛选手将在包含Stable Diffusion 2.0生成的各种（提示、图像）对的数据集上进行预测，通过了解潜在存在的提示、图像之间关系的可逆性。参赛选手通过构建一个模型来预测给定生成图像的文本提示。并把这个文本提示与标注过的文本提示进行对比。

![ques](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\ques.jpg)

**评价方式使用预测的和实际的提示嵌入向量之间的平均余弦相似性得分来评估提交。**对于测试集中的每个图像，必须预测用于生成图像的提示，并将提示转换为384长度的嵌入向量。预测应该被展平为图像（imgId）和嵌入（eId）对（imgId_eId）的行。

```python
imgId_eId,val
20057f34d_0,0.018848453
20057f34d_1,0.030189732
....
20057f34d_383,-0.007934112
227ef0887_0,0.017384542
etc.
```

我们的模型中用到的技术：

- CLIP，OpenAI 提出的多模态模型，原任务是zero-shot的图片分类。
- Vit-Large，Transformer在CV(计算机视觉)领域的应用，原来的任务是图片分类。
- LoRA，一种参数高效的微调方法。

## 深度学习中的图像描述

> 图像描述是生成图像文本描述的过程。它使用**自然语言处理**和**计算机视觉**来生成字幕。数据集的格式为 [**image** → **captions**]。数据集由输入图像及其相应的输出标题组成。

具有图像及其标题的流行基准数据集是：

- [上下文中的常见对象(COCO)](http://mscoco.org/dataset/#overview)超过 120,000 张带有描述的图像的集合。
- Flickr 8K：从 flickr.com 拍摄的 8000 张描述图像的集合。
- Flickr 30K：从 flickr.com 拍摄的 30,000 张描述图像的集合

一个基本的Network Topology：

![1](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\1.png)

卷积神经网络（CNN）可以被认为是一个编码器。输入图像被提供给CNN以提取特征。CNN 的最后一个隐藏状态连接到解码器。

解码器是一种递归神经网络（RNN），可以进行语言建模，直到单词级别。第一个时间步长接收来自编码器的编码输出以及向量。我们设置：

- x1 = ， y1 = 序列中的第一个单词。
- x2 =第一个单词的词向量，并期望网络预测第二个词。
- 在最后一步，xT = 最后一个字，目标标签 yT = 标记。

在训练过程中，即使在解码器之前犯了错误，也会在每个时间步向解码器提供正确的输入。

在测试期间，解码器在时间 t 的输出被反馈，并在时间 t+1 成为解码器的输入。

## MLP(多层感知机)

 多层感知机（MLP，Multilayer Perceptron）也叫[人工神经网络](https://so.csdn.net/so/search?q=人工神经网络&spm=1001.2101.3001.7020)（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构，如下图：

![20190623203530221](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\20190623203530221.png)

从上图可以看到，多层感知机层与层之间是全连接的。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。

隐藏层的神经元怎么得来？首先它与输入层是全连接的，假设输入层用向量X表示，则隐藏层的输出就是 f (W1X+b1)，W1是权重（也叫连接系数），b1是偏置，函数f 可以是常用的sigmoid函数或者tanh函数：

## Vision Transformer

简单而言，模型由三个模块组成：

![vit](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\vit.webp)

- Linear Projection of Flattened Patches(Embedding层)，对于图像数据而言，其数据格式为[H, W, C]是三维矩阵明显不是Transformer想要的。所以需要先通过一个Embedding 层来对数据做个变换。
- Transformer Encoder(图右侧有给出更加详细的结构)，即重复堆叠Encoder Block $L$次
- MLP Head（最终用于分类的层结构）

![vit](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\vit.gif)

在我们的模型中，使用的是 Large 版本

| Model     | Patch Size | Layers | Hidden Size D | MLP size | Heads | Params |
| --------- | ---------- | ------ | ------------- | -------- | ----- | ------ |
| Vit-Base  | 16*16      | 12     | 768           | 3072     | 12    | 86M    |
| Vit-Large | 16*16      | 24     | 1024          | 4096     | 16    | 307M   |
| Vit-Huge  | 14*14      | 32     | 1280          | 5120     | 16    | 632M   |

## CLIP

CLIP（Contrastive LanguageImage Pre-training）是OpenAI 提出的多模态模型，后续也作为基础 模型，被广泛用在DALLE2，Stable Diffusion等重要文生图大模型中， 实现了zero-shot的图片分类。

[Zero-shot](https://so.csdn.net/so/search?q=Zero-shot&spm=1001.2101.3001.7020)学习（Zero-shot Learning，ZSL）是深度学习领域中的一个重要研究方向，它旨在让机器学习模型能够在没有见过特定类别样本的情况下，对该类别进行识别或分类。换句话说，ZSL试图让模型能够泛化到训练时未见过的类别上。

### Zero-shot的应用

- **图像识别**：在图像识别领域，ZSL可以帮助模型识别训练集中未包含的动物、植物或其他对象。
- **自然语言处理**：在自然语言处理中，ZSL可以用于理解和分类未见过的词汇或概念。
- **跨领域学习**：ZSL允许模型在不同领域之间进行知识迁移，例如从视觉数据迁移到文本数据。

![CLIP](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\CLIP.png)

模型训练步骤：

1. 输入图片->图像编码器（vision transformer）->图片特征向量
2. 输入文字->文本编码器（text ）->文本特征向量
3. 对两个特征进行线性投射，得到相同维度的特征，并进行L2归一化
4. 计算两个特征向量的相似度（夹角余弦）
5. 对n个类别进行softmax，确定个正样本和个负样本，并最大化正样本的权重

模型预测步骤：

1. 给出一些文本提升，选项中要包含正确答案。
2. 然后计算每一个文本提升和图片特征的相似度。
3. 找到相似度最高的即为正确答案.

**CLIP由两个编码器组成，一个是图像编码器（VIT和ResNet，都用了），另一个是文本编码器（Transformer）**

本次模型中引入VIT主要通过利用CLIP引入基 本模型，之后又通过LoRA对VIT模型的 encoder中的attn进行改善微调，从而更好地 提升模型效果。

## Vit Gpt2 Image Captioning

[视觉编码器解码器](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder)模型可用于初始化图像到文本模型，使用任何预训练的基于 Transformer 的视觉模型作为编码器（例如 ViT、BEiT、DeiT、Swin）和任何预训练的语言模型作为解码器（例如 RoBERTa、GPT2、BERT、DistilBERT）。

![vision-encoder-decoder](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\vision-encoder-decoder.png)

`Image captioning`是一个示例，其中编码器模型用于对图像进行编码，然后使用自回归语言模型（即解码器模型）生成标题。

```python
image_encoder_model = "google/vit-base-patch16-224-in21k"  # 编码器模型用于对图像进行编码
text_decode_model = "gpt2"  # 自回归语言模型（即解码器模型）生成标题

model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(image_encoder_model, text_decode_model)
```

```python
model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")  # 创建模型
output_ids = model.generate(pixel_values, **gen_kwargs)  # 生成图片的描述
```

## LoRA

由于大语言模型参数量十分庞大，当将其应用到下游任务时，微调全部参数需要相当高的算力。为了节省成本，研究人员提出了多种参数高效（Parameter Efficient）的微调方法，旨在仅训练少量参数使模型适应到下游任务。LoRA （Low-Rank Adaptation of Large Language Models）方法 可以在缩减训练参数量和 GPU 显存占用的同时，使训练后的模型具有与全量微调相当的性能。

当微调语言模型时，我们会修改模型的基本参数。为了使这一想法更加具体，微调得出的参数可以有如下的形式化表达：

![277009976_1_20231217083020882_wm](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\277009976_1_20231217083020882_wm.webp)

LoRA的核心思想是通过低秩分解对模型参数的更新进行建模,在实践中实现为一对线性投影。LoRA使LLM的预训练层保持固定，并将可训练秩分解矩阵注入模型的每一层，如下图所示：

![LORA](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\LORA.png)

对于 GPT-3 模型，当 r = 4 且仅在注意力模块的 Q 矩阵和 V 矩 阵添加旁路时，保存的检查点大小减小了 10000 倍（从原本的 350GB 变为 35MB），训练时 GPU 显存占用从原本的 1.2TB 变为 350GB，训练速度相较全量参数微调提高 25%。

![t54](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\t54.png)

### 秩分解矩阵

简单而言，秩分解矩阵只是两个线性投影，它们减少并恢复输入的维度。这两个线性投影的输出被添加到从模型的预训练权重导出的输出中。通过添加这两个平行变换形成的更新层，LoRA被添加到直接学习底层权重并实现更新。

![277009976_3_2023121708302154_wm](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\277009976_3_2023121708302154_wm.webp)

矩阵乘积AB具有与完全微调更新相同的维度。将更新分解为两个较小矩阵的乘积可以确保更新是低秩的，并显著减少必须训练的参数数量。LoRA不是直接微调预训练LLM层中的参数，而是仅优化秩分解矩阵，产生近似于从完全微调中导出的更新结果。用随机的小值初始化A，而B初始化为零，可以确保用模型的原始预训练权重开始微调过程。

也就是说，通过将LoRA的秩r设置为预先训练的权重矩阵的秩，大致恢复了完全微调（fully finetuning）的表现力。增加r可以提高LoRA对完整微调更新的近似值，但在实践中，r的值非常小就足够了，这样能够在对性能影响最小的情况下显著降低计算和内存成本。例如，仅使用总参数的0.01%的LoRA来微调GPT-3，并且仍然可以实现与完全微调相当的性能。

## all-MiniLM-L6-v2

复习一下，一个基本的SBert模型如下：

![basic_sbert](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\basic_sbert.png)

Sentence-Transformers 提供了许多预训练模型，它们对嵌入式句子（Performance Sentence Embeddings）和嵌入式搜索查询和段落（Performance Semantic Search）的质量进行了广泛的评估。如下：

![sbmodels](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\sbmodels.png)

**all-*** 模型使用所有可用的训练数据（超过 10 亿个训练对）进行训练，并被设计为**通用**模型。**all-mpnet-base-v2** 型号提供最佳质量，而 **all-MiniLM-L6-v2** 速度提高 5 倍，并且仍然提供良好的质量。

是一个 sentence-transformers 模型：它将句子和段落映射到一个 384 维的密集向量空间，可用于聚类或语义搜索等任务。

可以像这样使用模型：

```python
from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = model.encode(sentences)
print(embeddings)
```

## 本模型

![model](C:\Users\zhangwenchao\Desktop\学习\人工智能算法竞赛\md图片\model.png)

在本模型中，我们借鉴了CLIP的结构，首先使用CLIP中的图片编码器Vit-large模型对图片进行Embedding和提取特征，然后通过自定义的MLP对图片特征进行语义提示生成。由于Vit-large模型中参数数量庞大，并且GPU训练时间较长，所以我们使用LoRA对Vit的注意力层（替代了60%的注意力层）进行缩减训练参数量和 GPU 显存占用，并且能够保证训练后的模型具有与全量微调相当的性能。

在具体模型部署中，参考了 OpenAI 在 Hugging Face 上发表的预训练模型 [clip-vit-large-patch14-336](https://huggingface.co/openai/clip-vit-large-patch14-336/tree/main)。

比赛模型`ClipNet`类，全连接层即`self.head`：

```python
class ClipNet(torch.nn.Module):
    def __init__(self, args, lora_rate=0.0, nfz_rate=0.0, lora_scale=1.0, rank=4):
        super().__init__()
        self.args = args
        config = CLIPConfig.from_pretrained(PRETRAINED_MODEL_PATH(args.model_name))
        config.vision_config.image_size = args.image_size
        clip = CLIPModel(config)

        pretrained_file = f'{PRETRAINED_MODEL_PATH(args.model_name)}/pytorch_model.pt'
        load_pretrained(clip, pretrained_file, strict=False, can_print=True)

        self.vision_model = clip.vision_model
        fc_dim = 16 * 1024
        ebd_dim = self.vision_model.embeddings.position_embedding.embedding_dim  

        # 添加的额外的全连接层，将输出的特征维度转换为embedding_dim，即384
        self.head = nn.Sequential(
            nn.Linear(ebd_dim, fc_dim),
            nn.BatchNorm1d(fc_dim),
            nn.ReLU(),
            nn.Linear(fc_dim, args.embedding_dim),
        )
        # Linear即全连接层
        lora_clip_model(self.vision_model, args, lora_scale, lora_rate, nfz_rate, rank)

    def forward(self, data):
        out = self.vision_model(data['image'])
        logits = self.head(out['pooler_output'])
        return logits
```

LoRA 注意力层的一些细节，在比赛模型中，LoRA层用于替换Transformer架构中， 自注意力模块中有四个权重矩阵 Q、K、V、O，具体来说，将视觉模型 self.vision_model 的后部 60% 层使用 LoRA 层替换：

```python
def lora_clip_model(model, args, lora_scale, lora_rate, nfz_rate=0.4, rank=4):
    # 在clip模型中，将lora_scale设置为1.0，lora_rate设置为0.6，nfz_rate设置为0.4，rank设置为4

    # 设置不计算梯度
    model.embeddings.requires_grad_(False) 
    model.pre_layrnorm.requires_grad_(False)  
    model.encoder.layers.requires_grad_(False)
    layer_num = len(model.encoder.layers)
    lora_num = int(layer_num * lora_rate)  # lora层数
    nfz_num = int(layer_num * nfz_rate)  # 非冻结区域的比例
    for _i in range(layer_num - lora_num - nfz_num, layer_num):  # 从倒数第lora_num层开始，到最后一层
        if (layer_num - _i) <= nfz_num:  # 如果是非冻结区域的比例
            model.encoder.layers[_i].requires_grad_(True)  # 设置计算梯度
            continue

        # 如果是lora层
        attn = model.encoder.layers[_i].self_attn  # 获取self_attn
        new_attn = LoRACLIPAttention(attn, rank=rank, lora_scale=lora_scale)  # 替换为lora attention
        model.encoder.layers[_i].self_attn = new_attn  # 替换
    if args.can_print: print(f'lora clip_model, scale:{lora_scale} rate:{lora_rate} num:{lora_num}')
```

